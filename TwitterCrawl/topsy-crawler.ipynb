{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import fileinput\n",
      "import json\n",
      "import csv\n",
      "import sys\n",
      "import urllib\n",
      "import urlparse\n",
      "import time\n",
      "\n",
      "class TopsyCrawler:\n",
      "    \"\"\"\n",
      "    Crawl Topsy\n",
      "    \"\"\"\n",
      "    API_KEY = \"\"\n",
      "    base_url = \"http://otter.topsy.com/search.json?q=\"\n",
      "    base_params = {}\n",
      "    csvHeadersWritten = False\n",
      "    \n",
      "    def __init__(self, api_key):\n",
      "        self.API_KEY = api_key\n",
      "    \n",
      "    def queryBuilder(self, **kwargs):\n",
      "        #Reference: http://stackoverflow.com/a/2506477/2762836\n",
      "        \"\"\"\n",
      "        Builds complete query string to be used to query Topsy API\n",
      "        Parameters:\n",
      "        \n",
      "        Window defines the time frame which is to be searched for tweets\n",
      "        window=h (last 1 hour)\n",
      "        window=d (last 1 day)\n",
      "        window=d5 (last 5 days)\n",
      "        window=w (last 7 days)\n",
      "        window=m (last month)\n",
      "        window=a (all time)\n",
      "        \n",
      "        \"\"\"\n",
      "        self.base_params = kwargs\n",
      "        url_parts = list(urlparse.urlparse(self.base_url))\n",
      "        query = dict(urlparse.parse_qsl(url_parts[4]))\n",
      "        query.update(self.base_params)\n",
      "        url_parts[4] = urllib.urlencode(query)\n",
      "        initial_url = urlparse.urlunparse(url_parts)\n",
      "        print initial_url\n",
      "        sys.stdout.flush()\n",
      "        return initial_url\n",
      "    \n",
      "    def crawlUrl(self, url):\n",
      "        \"\"\"\n",
      "        gets results from querying the url\n",
      "        \"\"\"\n",
      "        return json.loads(urllib.urlopen(url).readlines()[0]) #return json output\n",
      "    \n",
      "    def fetchTweets(self, maxTweetNumber, delayPerRequest, writeFileHandle, folderName, **kwargs):\n",
      "        \"\"\"\n",
      "        fetches tweets until the number of tweets fetched exceeds 'maxTweetNumber'\n",
      "        'delayPerRequest' is the time in seconds to wait before making next request.\n",
      "        \"\"\"\n",
      "        #Build first query\n",
      "        url = self.queryBuilder(**kwargs)\n",
      "        #First first page of results\n",
      "        resultObj = {}\n",
      "        resultObj = self.crawlUrl(url)\n",
      "        processedResult = ResultsProcessor(resultObj)\n",
      "        self.writeJsonToCsv(processedResult, resultObj, writeFileHandle, 0, kwargs['q'], folderName)\n",
      "        self.csvHeadersWritten = True\n",
      "        offset = processedResult.offset\n",
      "        nextOffset = offset+10\n",
      "        noOfTweetsFetched = len(processedResult.response['list'])\n",
      "        while True:\n",
      "            #Check if condition to exit the loop is met\n",
      "            if noOfTweetsFetched > maxTweetNumber:\n",
      "                break\n",
      "            if len(processedResult.response['list']) == 0:\n",
      "                break\n",
      "            #Wait for sometime before next request\n",
      "            time.sleep(delayPerRequest)\n",
      "            #Query the url\n",
      "            url = self.queryBuilder(apikey=self.base_params['apikey'], type=self.base_params['type'], window=self.base_params['window'], q=self.base_params['q'], offset=nextOffset)\n",
      "            resultObj = self.crawlUrl(url)\n",
      "            processedResult = ResultsProcessor(resultObj)\n",
      "            self.writeJsonToCsv(processedResult, resultObj, writeFileHandle, nextOffset, kwargs['q'], folderName)\n",
      "            \n",
      "            #Book keeping for processing next result\n",
      "            nextOffset = nextOffset+10\n",
      "            noOfTweetsFetched = len(processedResult.response['list']) + noOfTweetsFetched\n",
      "            \n",
      "    def writeJsonToCsv(self, jsonData, jsonRawData, writeFile, offset, queryTags, folderName):\n",
      "        \"\"\"\n",
      "        Used to write tweets data to csv file\n",
      "        writeFileHandle is False if output is to be written to stdout\n",
      "        writeFileHandle has fileName if output json is to be written to file\n",
      "        \"\"\"\n",
      "        if not writeFile:\n",
      "            columnNames  = ['hits','firstpost_date','title','url','trackback_date','trackback_total','url_expansions','target_birth_date','content',\\\n",
      "        'mytype','score','topsy_author_img','trackback_permalink','trackback_author_url','highlight','topsy_author_url','topsy_trackback_url','trackback_author_name','trackback_author_nick']\n",
      "            if self.csvHeadersWritten ==  False:\n",
      "                #Write column names at the top of the csv\n",
      "                print \"\\t\".join(columnNames)\n",
      "            \n",
      "            #For now, simplify\n",
      "            for tweet in jsonData.response['list']:\n",
      "                line = \"\"\n",
      "                for column in columnNames:\n",
      "                    if type(tweet[column])==unicode:\n",
      "                        #print \"string\"\n",
      "                        if column == 'trackback_author_nick':\n",
      "                            #This is the last column and so insert new line after it for the new tweet\n",
      "                            print repr(tweet[column].encode('utf-8'))+\"\\t\"\n",
      "                        else:\n",
      "                            print repr(tweet[column].encode('utf-8'))+\"\\t\",\n",
      "                    else:\n",
      "                        #print \"number\"\n",
      "                        print str(tweet[column])+\"\\t\",\n",
      "        else:\n",
      "            #write json output to file\n",
      "            myfp = open(folderName+\"/\"+\"_\".join(queryTags.lower().split())+\"_\"+ str(offset) +\".json\",\"w\")\n",
      "            json.dump(jsonRawData,  myfp, indent=4, sort_keys=False, separators=(',', ': '))\n",
      "            myfp.close()\n",
      "        \n",
      "class ResultsProcessor:\n",
      "    \"\"\"\n",
      "    This class will perform operations on json results received from Topsy Crawler class\n",
      "    \"\"\"\n",
      "    resultsJsonDictionary = {}\n",
      "    request = {}\n",
      "    response = {}\n",
      "    page = 0\n",
      "    window = \"\" #specify if data is to be fetched from last day(d5), week(w), month(m) or all time(a)\n",
      "    offset = 0\n",
      "    hidden = 0\n",
      "    total = 0\n",
      "    last_offset = 0\n",
      "    \n",
      "    def __init__(self, resultsJson):\n",
      "        self.resultsJsonDictionary = resultsJson #convert string into valid json format\n",
      "        self.request = self.resultsJsonDictionary['request']\n",
      "        self.response = self.resultsJsonDictionary['response']\n",
      "        self.page = self.resultsJsonDictionary['response']['page']\n",
      "        self.window = self.resultsJsonDictionary['response']['window']\n",
      "        self.offset = self.resultsJsonDictionary['response']['offset']\n",
      "        self.hidden = self.resultsJsonDictionary['response']['hidden']\n",
      "        self.total = self.resultsJsonDictionary['response']['total']\n",
      "        self.last_offset = self.resultsJsonDictionary['response']['last_offset']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    API_KEY = \"09C43A9B270A470B8EB8F2946A9369F3\"\n",
      "    \"\"\"\n",
      "    #Example to examine tweets\n",
      "    crawlerObj = TopsyCrawler(API_KEY)\n",
      "    url = crawlerObj.queryBuilder(apikey=API_KEY, type='tweet', window='a', q='#happy #surprised')\n",
      "    jsonResults = crawlerObj.crawlUrl(url)\n",
      "    resultsObj = ResultsProcessor(jsonResults)\n",
      "    \"\"\"\n",
      "    #example to fetch multiple tweets\n",
      "    crawlerObj = TopsyCrawler(API_KEY)\n",
      "    crawlerObj.fetchTweets(10, 5, True, \"./\", apikey=API_KEY, type='tweet', window='a', q='\u0634\u0648\u0641')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'TopsyCrawler' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-d5c6d597181f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#example to fetch multiple tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mcrawlerObj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTopsyCrawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPI_KEY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mcrawlerObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchTweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapikey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAPI_KEY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\u0634\u0648\u0641'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'TopsyCrawler' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = json.load(open(\"hash_happy_hash_suprised.json\",\"r\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crawlerObj = TopsyCrawler(API_KEY)\n",
      "crawlerObj.fetchTweets(10, 5, True, \".\", apikey=API_KEY, type='tweet', window='a', q='#happy #surprised')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://otter.topsy.com/search.json?q=%23happy+%23surprised&window=a&apikey=09C43A9B270A470B8EB8F2946A9369F3&type=tweet\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://otter.topsy.com/search.json?q=%23happy+%23surprised&window=a&apikey=09C43A9B270A470B8EB8F2946A9369F3&type=tweet&offset=10\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"hello\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hello\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "folders = [\"#happily happy\",\n",
      "\"happily #happy\",\n",
      "\"happily happy\",\n",
      "\n",
      "\"#happily #sad\",\n",
      "\"happily #sad\",\n",
      "\"#happily sad\",\n",
      "\"happily sad\",\n",
      "\n",
      "\"#happily #angry\",\n",
      "\"#happily angry\",\n",
      "\"happily #angry\",\n",
      "\"happily angry\",\n",
      "\n",
      "\"#happily #fearful\",\n",
      "\"happily #fearful\",\n",
      "\"#happily fearful\",\n",
      "\"happily fearful\",\n",
      "\n",
      "\"#happily #surprised\",\n",
      "\"happily #surprised\",\n",
      "\"#happily surprised\",\n",
      "\"happily surprised\",\n",
      "\n",
      "\"#happily #disgusted\",\n",
      "\"happily #disgusted\",\n",
      "\"#happily disgusted\",\n",
      "\"happily disgusted\",\n",
      "\n",
      "\"#surprisingly #happy\",\n",
      "\"surprisingly #happy\", \n",
      "\"#surprisingly happy\", \n",
      "\"surprisingly happy\", \n",
      "\n",
      "\"#surprisingly #sad\",\n",
      "\"surprisingly #sad\",\n",
      "\"#surprisingly sad\",\n",
      "\"surprisingly sad\",\n",
      "\n",
      "\"#surprisingly #angry\",\n",
      "\"surprisingly angry\",\n",
      "\"surprisingly #angry\",\n",
      "\"surprisingly angry\",\n",
      "\n",
      "\"#surprisingly #fearful\",\n",
      "\"surprisingly #fearful\",\n",
      "\"#surprisingly fearful\",\n",
      "\"surprisingly fearful\",\n",
      "\n",
      "\"#surprisingly #surprised\",\n",
      "\"surprisingly #surprised\",\n",
      "\"#surprisingly surprised\",\n",
      "\"surprisingly surprised\",\n",
      "\n",
      "\"#surprisingly #disgusted\",\n",
      "\"surprisingly #disgusted\",\n",
      "\"#surprisingly disgusted\",\n",
      "\"surprisingly disgusted\",\n",
      "\n",
      "\"#sadly #sad\",\n",
      "\"sadly #sad\",\n",
      "\"#sadly sad\",\n",
      "\"sadly sad\",\n",
      "\n",
      "\"#sadly #happy\",\n",
      "\"sadly #happy\",\n",
      "\"#sadly happy\",\n",
      "\"sadly happy\",\n",
      "\n",
      "\"#sadly #angry\",\n",
      "\"sadly angry\",\n",
      "\"sadly #angry\",\n",
      "\"sadly angry\",\n",
      "\n",
      "\"#sadly #fearful\",\n",
      "\"sadly #fearful\",\n",
      "\"#sadly fearful\",\n",
      "\"sadly fearful\",\n",
      "\n",
      "\"#sadly #surprised\",\n",
      "\"sadly #surprised\",\n",
      "\"#sadly surprised\",\n",
      "\"sadly surprised\",\n",
      "\n",
      "\"#sadly #disgusted\",\n",
      "\"sadly #disgusted\",\n",
      "\"#sadly disgusted\",\n",
      "\"sadly disgusted\",\n",
      "\n",
      "\"#angrily #angry\",\n",
      "\"angrily angry\",\n",
      "\"angrily #angry\",\n",
      "\"angrily angry\",\n",
      "\n",
      "\"#angrily #sad\",\n",
      "\"angrily #sad\",\n",
      "\"#angrily sad\",\n",
      "\"angrily sad\",\n",
      "\n",
      "\"#angrily #happy\",\n",
      "\"angrily #happy\",\n",
      "\"#angrily happy\",\n",
      "\"angrily happy\",\n",
      "\n",
      "\"#angrily #fearful\",\n",
      "\"angrily #fearful\",\n",
      "\"#angrily fearful\",\n",
      "\"angrily fearful\",\n",
      "\n",
      "\"#angrily #surprised\",\n",
      "\"angrily #surprised\",\n",
      "\"#angrily surprised\",\n",
      "\"angrily surprised\",\n",
      "\n",
      "\"#angrily #disgusted\",\n",
      "\"angrily #disgusted\",\n",
      "\"#angrily disgusted\",\n",
      "\"angrily disgusted\",\n",
      "\n",
      "\"#fearfully #fearful\",\n",
      "\"fearfully #fearful\",\n",
      "\"#fearfully fearful\",\n",
      "\"fearfully fearful\",\n",
      "\n",
      "\"#fearfully #angry\",\n",
      "\"fearfully angry\",\n",
      "\"fearfully #angry\",\n",
      "\"fearfully angry\",\n",
      "\n",
      "\"#fearfully #sad\",\n",
      "\"fearfully #sad\",\n",
      "\"#fearfully sad\",\n",
      "\"fearfully sad\",\n",
      "\n",
      "\"#fearfully #happy\",\n",
      "\"fearfully #happy\",\n",
      "\"#fearfully happy\",\n",
      "\"fearfully happy\",\n",
      "\n",
      "\"#fearfully #surprised\",\n",
      "\"fearfully #surprised\",\n",
      "\"#fearfully surprised\",\n",
      "\"fearfully surprised\",\n",
      "\n",
      "\"#fearfully #disgusted\",\n",
      "\"fearfully #disgusted\",\n",
      "\"#fearfully disgusted\",\n",
      "\"fearfully disgusted\",\n",
      "\n",
      "\"#disgusted #disgusted\",\n",
      "\"disgusted #disgusted\",\n",
      "\"#disgusted disgusted\",\n",
      "\"disgusted disgusted\",\n",
      "\n",
      "\"#disgusted #angry\",\n",
      "\"disgusted angry\",\n",
      "\"disgusted #angry\",\n",
      "\"disgusted angry\",\n",
      "\n",
      "\"#disgusted #sad\",\n",
      "\"disgusted #sad\",\n",
      "\"#disgusted sad\",\n",
      "\"disgusted sad\",\n",
      "\n",
      "\"#disgusted #happy\",\n",
      "\"disgusted #happy\",\n",
      "\"#disgusted happy\",\n",
      "\"disgusted happy\",\n",
      "\n",
      "\"#disgusted #fearful\",\n",
      "\"disgusted #fearful\",\n",
      "\"#disgusted fearful\",\n",
      "\"disgusted fearful\",\n",
      "\n",
      "\"#disgusted #surprised\",\n",
      "\"disgusted #surprised\",\n",
      "\"#disgusted surprised\",\n",
      "\"disgusted surprised\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for folder in folders:\n",
      "    query = folder\n",
      "    path = \"'\"+\"_\".join(folder.split())+\"'\"\n",
      "    #print query\n",
      "    print \"nice python topsy-crawler.py 1000 10 True CrawledData/\"+path.strip(\"'\")+ \" '\"+query+\"'\"+ \" > logs/console_log_\"+path.strip(\"'\")+\".txt\"\n",
      "    #print path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "nice python topsy-crawler.py 1000 10 True CrawledData/#happily_happy '#happily happy' > logs/console_log_#happily_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_#happy 'happily #happy' > logs/console_log_happily_#happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_happy 'happily happy' > logs/console_log_happily_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#happily_#sad '#happily #sad' > logs/console_log_#happily_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_#sad 'happily #sad' > logs/console_log_happily_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#happily_sad '#happily sad' > logs/console_log_#happily_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_sad 'happily sad' > logs/console_log_happily_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#happily_#angry '#happily #angry' > logs/console_log_#happily_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#happily_angry '#happily angry' > logs/console_log_#happily_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_#angry 'happily #angry' > logs/console_log_happily_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_angry 'happily angry' > logs/console_log_happily_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#happily_#fearful '#happily #fearful' > logs/console_log_#happily_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_#fearful 'happily #fearful' > logs/console_log_happily_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#happily_fearful '#happily fearful' > logs/console_log_#happily_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_fearful 'happily fearful' > logs/console_log_happily_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#happily_#surprised '#happily #surprised' > logs/console_log_#happily_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_#surprised 'happily #surprised' > logs/console_log_happily_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#happily_surprised '#happily surprised' > logs/console_log_#happily_surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_surprised 'happily surprised' > logs/console_log_happily_surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#happily_#disgusted '#happily #disgusted' > logs/console_log_#happily_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_#disgusted 'happily #disgusted' > logs/console_log_happily_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#happily_disgusted '#happily disgusted' > logs/console_log_#happily_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/happily_disgusted 'happily disgusted' > logs/console_log_happily_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#surprisingly_#happy '#surprisingly #happy' > logs/console_log_#surprisingly_#happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_#happy 'surprisingly #happy' > logs/console_log_surprisingly_#happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#surprisingly_happy '#surprisingly happy' > logs/console_log_#surprisingly_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_happy 'surprisingly happy' > logs/console_log_surprisingly_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#surprisingly_#sad '#surprisingly #sad' > logs/console_log_#surprisingly_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_#sad 'surprisingly #sad' > logs/console_log_surprisingly_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#surprisingly_sad '#surprisingly sad' > logs/console_log_#surprisingly_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_sad 'surprisingly sad' > logs/console_log_surprisingly_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#surprisingly_#angry '#surprisingly #angry' > logs/console_log_#surprisingly_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_angry 'surprisingly angry' > logs/console_log_surprisingly_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_#angry 'surprisingly #angry' > logs/console_log_surprisingly_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_angry 'surprisingly angry' > logs/console_log_surprisingly_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#surprisingly_#fearful '#surprisingly #fearful' > logs/console_log_#surprisingly_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_#fearful 'surprisingly #fearful' > logs/console_log_surprisingly_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#surprisingly_fearful '#surprisingly fearful' > logs/console_log_#surprisingly_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_fearful 'surprisingly fearful' > logs/console_log_surprisingly_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#surprisingly_#surprised '#surprisingly #surprised' > logs/console_log_#surprisingly_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_#surprised 'surprisingly #surprised' > logs/console_log_surprisingly_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#surprisingly_surprised '#surprisingly surprised' > logs/console_log_#surprisingly_surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_surprised 'surprisingly surprised' > logs/console_log_surprisingly_surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#surprisingly_#disgusted '#surprisingly #disgusted' > logs/console_log_#surprisingly_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_#disgusted 'surprisingly #disgusted' > logs/console_log_surprisingly_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#surprisingly_disgusted '#surprisingly disgusted' > logs/console_log_#surprisingly_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/surprisingly_disgusted 'surprisingly disgusted' > logs/console_log_surprisingly_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#sadly_#sad '#sadly #sad' > logs/console_log_#sadly_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_#sad 'sadly #sad' > logs/console_log_sadly_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#sadly_sad '#sadly sad' > logs/console_log_#sadly_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_sad 'sadly sad' > logs/console_log_sadly_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#sadly_#happy '#sadly #happy' > logs/console_log_#sadly_#happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_#happy 'sadly #happy' > logs/console_log_sadly_#happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#sadly_happy '#sadly happy' > logs/console_log_#sadly_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_happy 'sadly happy' > logs/console_log_sadly_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#sadly_#angry '#sadly #angry' > logs/console_log_#sadly_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_angry 'sadly angry' > logs/console_log_sadly_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_#angry 'sadly #angry' > logs/console_log_sadly_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_angry 'sadly angry' > logs/console_log_sadly_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#sadly_#fearful '#sadly #fearful' > logs/console_log_#sadly_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_#fearful 'sadly #fearful' > logs/console_log_sadly_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#sadly_fearful '#sadly fearful' > logs/console_log_#sadly_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_fearful 'sadly fearful' > logs/console_log_sadly_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#sadly_#surprised '#sadly #surprised' > logs/console_log_#sadly_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_#surprised 'sadly #surprised' > logs/console_log_sadly_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#sadly_surprised '#sadly surprised' > logs/console_log_#sadly_surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_surprised 'sadly surprised' > logs/console_log_sadly_surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#sadly_#disgusted '#sadly #disgusted' > logs/console_log_#sadly_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_#disgusted 'sadly #disgusted' > logs/console_log_sadly_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#sadly_disgusted '#sadly disgusted' > logs/console_log_#sadly_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/sadly_disgusted 'sadly disgusted' > logs/console_log_sadly_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#angrily_#angry '#angrily #angry' > logs/console_log_#angrily_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_angry 'angrily angry' > logs/console_log_angrily_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_#angry 'angrily #angry' > logs/console_log_angrily_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_angry 'angrily angry' > logs/console_log_angrily_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#angrily_#sad '#angrily #sad' > logs/console_log_#angrily_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_#sad 'angrily #sad' > logs/console_log_angrily_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#angrily_sad '#angrily sad' > logs/console_log_#angrily_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_sad 'angrily sad' > logs/console_log_angrily_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#angrily_#happy '#angrily #happy' > logs/console_log_#angrily_#happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_#happy 'angrily #happy' > logs/console_log_angrily_#happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#angrily_happy '#angrily happy' > logs/console_log_#angrily_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_happy 'angrily happy' > logs/console_log_angrily_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#angrily_#fearful '#angrily #fearful' > logs/console_log_#angrily_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_#fearful 'angrily #fearful' > logs/console_log_angrily_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#angrily_fearful '#angrily fearful' > logs/console_log_#angrily_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_fearful 'angrily fearful' > logs/console_log_angrily_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#angrily_#surprised '#angrily #surprised' > logs/console_log_#angrily_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_#surprised 'angrily #surprised' > logs/console_log_angrily_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#angrily_surprised '#angrily surprised' > logs/console_log_#angrily_surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_surprised 'angrily surprised' > logs/console_log_angrily_surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#angrily_#disgusted '#angrily #disgusted' > logs/console_log_#angrily_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_#disgusted 'angrily #disgusted' > logs/console_log_angrily_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#angrily_disgusted '#angrily disgusted' > logs/console_log_#angrily_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/angrily_disgusted 'angrily disgusted' > logs/console_log_angrily_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#fearfully_#fearful '#fearfully #fearful' > logs/console_log_#fearfully_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_#fearful 'fearfully #fearful' > logs/console_log_fearfully_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#fearfully_fearful '#fearfully fearful' > logs/console_log_#fearfully_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_fearful 'fearfully fearful' > logs/console_log_fearfully_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#fearfully_#angry '#fearfully #angry' > logs/console_log_#fearfully_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_angry 'fearfully angry' > logs/console_log_fearfully_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_#angry 'fearfully #angry' > logs/console_log_fearfully_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_angry 'fearfully angry' > logs/console_log_fearfully_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#fearfully_#sad '#fearfully #sad' > logs/console_log_#fearfully_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_#sad 'fearfully #sad' > logs/console_log_fearfully_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#fearfully_sad '#fearfully sad' > logs/console_log_#fearfully_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_sad 'fearfully sad' > logs/console_log_fearfully_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#fearfully_#happy '#fearfully #happy' > logs/console_log_#fearfully_#happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_#happy 'fearfully #happy' > logs/console_log_fearfully_#happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#fearfully_happy '#fearfully happy' > logs/console_log_#fearfully_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_happy 'fearfully happy' > logs/console_log_fearfully_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#fearfully_#surprised '#fearfully #surprised' > logs/console_log_#fearfully_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_#surprised 'fearfully #surprised' > logs/console_log_fearfully_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#fearfully_surprised '#fearfully surprised' > logs/console_log_#fearfully_surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_surprised 'fearfully surprised' > logs/console_log_fearfully_surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#fearfully_#disgusted '#fearfully #disgusted' > logs/console_log_#fearfully_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_#disgusted 'fearfully #disgusted' > logs/console_log_fearfully_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#fearfully_disgusted '#fearfully disgusted' > logs/console_log_#fearfully_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/fearfully_disgusted 'fearfully disgusted' > logs/console_log_fearfully_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#disgusted_#disgusted '#disgusted #disgusted' > logs/console_log_#disgusted_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_#disgusted 'disgusted #disgusted' > logs/console_log_disgusted_#disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#disgusted_disgusted '#disgusted disgusted' > logs/console_log_#disgusted_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_disgusted 'disgusted disgusted' > logs/console_log_disgusted_disgusted.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#disgusted_#angry '#disgusted #angry' > logs/console_log_#disgusted_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_angry 'disgusted angry' > logs/console_log_disgusted_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_#angry 'disgusted #angry' > logs/console_log_disgusted_#angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_angry 'disgusted angry' > logs/console_log_disgusted_angry.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#disgusted_#sad '#disgusted #sad' > logs/console_log_#disgusted_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_#sad 'disgusted #sad' > logs/console_log_disgusted_#sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#disgusted_sad '#disgusted sad' > logs/console_log_#disgusted_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_sad 'disgusted sad' > logs/console_log_disgusted_sad.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#disgusted_#happy '#disgusted #happy' > logs/console_log_#disgusted_#happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_#happy 'disgusted #happy' > logs/console_log_disgusted_#happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#disgusted_happy '#disgusted happy' > logs/console_log_#disgusted_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_happy 'disgusted happy' > logs/console_log_disgusted_happy.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#disgusted_#fearful '#disgusted #fearful' > logs/console_log_#disgusted_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_#fearful 'disgusted #fearful' > logs/console_log_disgusted_#fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#disgusted_fearful '#disgusted fearful' > logs/console_log_#disgusted_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_fearful 'disgusted fearful' > logs/console_log_disgusted_fearful.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#disgusted_#surprised '#disgusted #surprised' > logs/console_log_#disgusted_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_#surprised 'disgusted #surprised' > logs/console_log_disgusted_#surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/#disgusted_surprised '#disgusted surprised' > logs/console_log_#disgusted_surprised.txt\n",
        "nice python topsy-crawler.py 1000 10 True CrawledData/disgusted_surprised 'disgusted surprised' > logs/console_log_disgusted_surprised.txt\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Writing a tab separated data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#imports\n",
      "import sys\n",
      "import json\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Read file from directory\n",
      "#dirName = sys.argv[1]\n",
      "dirName = \"./CrawledData-29May-1week/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Read json contents\n",
      "directories = os.listdir(dirName)\n",
      "listOfDataRows = []\n",
      "for directory in directories:\n",
      "    #Fetch each json in the current directory:\n",
      "    try:\n",
      "        for jsonFile  in os.listdir(dirName.strip(\"/\")+\"/\"+directory):\n",
      "            fileName = dirName.strip(\"/\")+\"/\"+directory+\"/\"+jsonFile\n",
      "            jsonData = json.load(open(fileName,\"r\"))\n",
      "            tabData = tabSeparatedData(jsonData)\n",
      "            if tabData:\n",
      "                listOfDataRows.extend(tabData)\n",
      "        #Now write this data to file\n",
      "        fh = open(\"./CrawledData-29May-1week-CSV/\"+directory+\".csv\",\"w\")\n",
      "        fh.write(\"firstpost_date\\turl\\turl_expansion_data_url\\turl_expansion_data_topsy_expanded_url\\tcontent\\ttrackback_author_nick\\ttrackback_author_name\\n\")\n",
      "        for row in listOfDataRows:\n",
      "            #print type(row)#.encode(\"utf-8\")\n",
      "            fh.write(row.encode('utf-8'))\n",
      "            fh.write(\"\\n\")\n",
      "        fh.close()\n",
      "    except:\n",
      "        print \"Error:\",sys.exc_info()[1]\n",
      "        continue"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Error: [Errno 20] Not a directory: './CrawledData-29May-1week/createDirectoriesScript.sh'\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Make each json data into tab separated format\n",
      "def tabSeparatedData(data):\n",
      "    listOfDataRows = []\n",
      "    try:\n",
      "        for item in data['response']['list']:\n",
      "            dataRow = str(item['firstpost_date'])\n",
      "            dataRow = dataRow + \"\\t\"+ item['url']\n",
      "            #if url expansions exist:\n",
      "            try:\n",
      "                url_expansion_data_url = item['url_expansions']['url'].encode(\"utf-8\")\n",
      "            except:\n",
      "                url_expansion_data_url = \"Not Available\"\n",
      "            try:\n",
      "                url_expansion_data_topsy_expanded_url = item['url_expansions']['topsy_expanded_url'].encode(\"utf-8\")\n",
      "            except:\n",
      "                url_expansion_data_topsy_expanded_url = \"Not Available\"\n",
      "            dataRow = dataRow + \"\\t\" + url_expansion_data_url.strip(\"\\t\")\n",
      "            dataRow = dataRow + \"\\t\" + url_expansion_data_topsy_expanded_url.strip(\"\\t\")\n",
      "            try:\n",
      "                dataRow = dataRow + \"\\t\" + repr(item['content']).encode(\"utf-8\")\n",
      "            except:\n",
      "                dataRow = dataRow + \"\\t\" + repr(item['content'])\n",
      "            try:\n",
      "                dataRow = dataRow + \"\\t\" + item['trackback_author_nick'].encode(\"utf-8\")\n",
      "            except:\n",
      "                dataRow = dataRow + \"\\t\" + item['trackback_author_nick']\n",
      "            try:\n",
      "                dataRow = dataRow + \"\\t\" + item['trackback_author_name'].encode(\"utf-8\")\n",
      "            except:\n",
      "                dataRow = dataRow + \"\\t\" + item['trackback_author_name']\n",
      "            listOfDataRows.append(dataRow)\n",
      "    except:\n",
      "        listOfDataRows = None\n",
      "    return listOfDataRows"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}