{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import fileinput\n",
      "import json\n",
      "import csv\n",
      "import sys\n",
      "import urllib\n",
      "import urlparse\n",
      "import time\n",
      "\n",
      "class TopsyCrawler:\n",
      "    \"\"\"\n",
      "    Crawl Topsy\n",
      "    \"\"\"\n",
      "    API_KEY = \"\"\n",
      "    base_url = \"http://otter.topsy.com/search.json?q=\"\n",
      "    base_params = {}\n",
      "    csvHeadersWritten = False\n",
      "    \n",
      "    def __init__(self, api_key):\n",
      "        self.API_KEY = api_key\n",
      "    \n",
      "    def queryBuilder(self, **kwargs):\n",
      "        #Reference: http://stackoverflow.com/a/2506477/2762836\n",
      "        \"\"\"\n",
      "        Builds complete query string to be used to query Topsy API\n",
      "        Parameters:\n",
      "        \n",
      "        Window defines the time frame which is to be searched for tweets\n",
      "        window=h (last 1 hour)\n",
      "        window=d (last 1 day)\n",
      "        window=d5 (last 5 days)\n",
      "        window=w (last 7 days)\n",
      "        window=m (last month)\n",
      "        window=a (all time)\n",
      "        \n",
      "        \"\"\"\n",
      "        self.base_params = kwargs\n",
      "        url_parts = list(urlparse.urlparse(self.base_url))\n",
      "        query = dict(urlparse.parse_qsl(url_parts[4]))\n",
      "        query.update(self.base_params)\n",
      "        url_parts[4] = urllib.urlencode(query)\n",
      "        initial_url = urlparse.urlunparse(url_parts)\n",
      "        print initial_url\n",
      "        sys.stdout.flush()\n",
      "        return initial_url\n",
      "    \n",
      "    def crawlUrl(self, url):\n",
      "        \"\"\"\n",
      "        gets results from querying the url\n",
      "        \"\"\"\n",
      "        return json.loads(urllib.urlopen(url).readlines()[0]) #return json output\n",
      "    \n",
      "    def fetchTweets(self, maxTweetNumber, delayPerRequest, writeFileHandle, folderName, **kwargs):\n",
      "        \"\"\"\n",
      "        fetches tweets until the number of tweets fetched exceeds 'maxTweetNumber'\n",
      "        'delayPerRequest' is the time in seconds to wait before making next request.\n",
      "        \"\"\"\n",
      "        #Build first query\n",
      "        url = self.queryBuilder(**kwargs)\n",
      "        #First first page of results\n",
      "        resultObj = {}\n",
      "        resultObj = self.crawlUrl(url)\n",
      "        processedResult = ResultsProcessor(resultObj)\n",
      "        self.writeJsonToCsv(processedResult, resultObj, writeFileHandle, 0, kwargs['q'], folderName)\n",
      "        self.csvHeadersWritten = True\n",
      "        offset = processedResult.offset\n",
      "        nextOffset = offset+10\n",
      "        noOfTweetsFetched = len(processedResult.response['list'])\n",
      "        while True:\n",
      "            #Check if condition to exit the loop is met\n",
      "            if noOfTweetsFetched > maxTweetNumber:\n",
      "                break\n",
      "            if len(processedResult.response['list']) == 0:\n",
      "                break\n",
      "            #Wait for sometime before next request\n",
      "            time.sleep(delayPerRequest)\n",
      "            #Query the url\n",
      "            url = self.queryBuilder(apikey=self.base_params['apikey'], type=self.base_params['type'], window=self.base_params['window'], q=self.base_params['q'], offset=nextOffset)\n",
      "            resultObj = self.crawlUrl(url)\n",
      "            processedResult = ResultsProcessor(resultObj)\n",
      "            self.writeJsonToCsv(processedResult, resultObj, writeFileHandle, nextOffset, kwargs['q'], folderName)\n",
      "            \n",
      "            #Book keeping for processing next result\n",
      "            nextOffset = nextOffset+10\n",
      "            noOfTweetsFetched = len(processedResult.response['list']) + noOfTweetsFetched\n",
      "            \n",
      "    def writeJsonToCsv(self, jsonData, jsonRawData, writeFile, offset, queryTags, folderName):\n",
      "        \"\"\"\n",
      "        Used to write tweets data to csv file\n",
      "        writeFileHandle is False if output is to be written to stdout\n",
      "        writeFileHandle has fileName if output json is to be written to file\n",
      "        \"\"\"\n",
      "        if not writeFile:\n",
      "            columnNames  = ['hits','firstpost_date','title','url','trackback_date','trackback_total','url_expansions','target_birth_date','content',\\\n",
      "        'mytype','score','topsy_author_img','trackback_permalink','trackback_author_url','highlight','topsy_author_url','topsy_trackback_url','trackback_author_name','trackback_author_nick']\n",
      "            if self.csvHeadersWritten ==  False:\n",
      "                #Write column names at the top of the csv\n",
      "                print \"\\t\".join(columnNames)\n",
      "            \n",
      "            #For now, simplify\n",
      "            for tweet in jsonData.response['list']:\n",
      "                line = \"\"\n",
      "                for column in columnNames:\n",
      "                    if type(tweet[column])==unicode:\n",
      "                        #print \"string\"\n",
      "                        if column == 'trackback_author_nick':\n",
      "                            #This is the last column and so insert new line after it for the new tweet\n",
      "                            print repr(tweet[column].encode('utf-8'))+\"\\t\"\n",
      "                        else:\n",
      "                            print repr(tweet[column].encode('utf-8'))+\"\\t\",\n",
      "                    else:\n",
      "                        #print \"number\"\n",
      "                        print str(tweet[column])+\"\\t\",\n",
      "        else:\n",
      "            #write json output to file\n",
      "            myfp = open(folderName+\"/\"+\"_\".join(queryTags.lower().split())+\"_\"+ str(offset) +\".json\",\"w\")\n",
      "            json.dump(jsonRawData,  myfp, indent=4, sort_keys=False, separators=(',', ': '))\n",
      "            myfp.close()\n",
      "        \n",
      "class ResultsProcessor:\n",
      "    \"\"\"\n",
      "    This class will perform operations on json results received from Topsy Crawler class\n",
      "    \"\"\"\n",
      "    resultsJsonDictionary = {}\n",
      "    request = {}\n",
      "    response = {}\n",
      "    page = 0\n",
      "    window = \"\" #specify if data is to be fetched from last day(d5), week(w), month(m) or all time(a)\n",
      "    offset = 0\n",
      "    hidden = 0\n",
      "    total = 0\n",
      "    last_offset = 0\n",
      "    \n",
      "    def __init__(self, resultsJson):\n",
      "        self.resultsJsonDictionary = resultsJson #convert string into valid json format\n",
      "        self.request = self.resultsJsonDictionary['request']\n",
      "        self.response = self.resultsJsonDictionary['response']\n",
      "        self.page = self.resultsJsonDictionary['response']['page']\n",
      "        self.window = self.resultsJsonDictionary['response']['window']\n",
      "        self.offset = self.resultsJsonDictionary['response']['offset']\n",
      "        self.hidden = self.resultsJsonDictionary['response']['hidden']\n",
      "        self.total = self.resultsJsonDictionary['response']['total']\n",
      "        self.last_offset = self.resultsJsonDictionary['response']['last_offset']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    API_KEY = \"09C43A9B270A470B8EB8F2946A9369F3\"\n",
      "    \"\"\"\n",
      "    #Example to examine tweets\n",
      "    crawlerObj = TopsyCrawler(API_KEY)\n",
      "    url = crawlerObj.queryBuilder(apikey=API_KEY, type='tweet', window='a', q='#happy #surprised')\n",
      "    jsonResults = crawlerObj.crawlUrl(url)\n",
      "    resultsObj = ResultsProcessor(jsonResults)\n",
      "    \"\"\"\n",
      "    #example to fetch multiple tweets\n",
      "    crawlerObj = TopsyCrawler(API_KEY)\n",
      "    crawlerObj.fetchTweets(2000, 5, True, \"CrawledData/#Happy#Surprised\", apikey=API_KEY, type='tweet', window='a', q='#happy #surprised')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = json.load(open(\"hash_happy_hash_suprised.json\",\"r\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crawlerObj = TopsyCrawler(API_KEY)\n",
      "crawlerObj.fetchTweets(10, 5, True, \".\", apikey=API_KEY, type='tweet', window='a', q='#happy #surprised')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://otter.topsy.com/search.json?q=%23happy+%23surprised&window=a&apikey=09C43A9B270A470B8EB8F2946A9369F3&type=tweet\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://otter.topsy.com/search.json?q=%23happy+%23surprised&window=a&apikey=09C43A9B270A470B8EB8F2946A9369F3&type=tweet&offset=10\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"hello\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hello\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}